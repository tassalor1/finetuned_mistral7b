{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aaf8153-e4d9-457f-b511-d2d006460515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb, os\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad74f9-5705-4d87-ace8-da0fcd877078",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "\n",
    "data_set = load_dataset('json', data_files=\"D:\\\\coding\\\\llms\\\\format_sci_fi_data2.jsonl\", split='train')\n",
    "\n",
    "lora_target_modules = [    # which layers to apply lora to\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ]\n",
    "\n",
    "lora_dropout = 0.05 # Dropout for lora weights to avoid overfitting\n",
    "lora_bias = \"none\"\n",
    "lora_r=32, # Bottleneck size between A and B matrix for lora params\n",
    "lora_alpha=64 # how much to weigh LoRA params over pretrained params\n",
    "\n",
    "project = \"sci-fi-finetune\"\n",
    "base_model_name = \"mistral7b\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "warmup_steps=1,\n",
    "per_device_train_batch_size=4,\n",
    "gradient_accumulation_steps=1,\n",
    "max_steps=500,\n",
    "learning_rate=0.00005, \n",
    "bf16=True,\n",
    "optim_type =\"paged_adamw_8bit\", # optimizer\n",
    "logging_steps=25,              # When to start reporting loss\n",
    "logging_dir=\"./logs\",        \n",
    "save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "save_steps=25,                # Save checkpoints every 25 steps\n",
    "evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "eval_steps=25,               # Evaluate and save checkpoints every 25 steps\n",
    "report_to=\"wandb\",          \n",
    "run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff28ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwalkerconnor14\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_project = \"mistral7b-finetune\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f560a36b-594f-471e-a535-ae2e82079134",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "use 4bit quantization on model \n",
    "use of double quant to try and retain the loss info\n",
    "from downsizing 32bit to 4bit\n",
    "'''\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization=bnb_config, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3cb01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define tokenizer with padding and eos/ bos tokens'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0c4e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3702fb4f8cc74435be1ef5abea7f8233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "''' tokenize dataset '''\n",
    "def generate_and_tokenize_prompt(entry):\n",
    "    combined_messages = ' '.join(f\"[{message['role']}] {message['content']}\" for message in entry['messages'])\n",
    "    tokenized_entry = tokenizer(\n",
    "        combined_messages,\n",
    "        truncation=True\n",
    "    )\n",
    "    tokenized_entry[\"labels\"] = tokenized_entry[\"input_ids\"].copy()\n",
    "    return tokenized_entry\n",
    "\n",
    "tokenized_data_set = data_set.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d597f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': 'You are the greatest sci-fi story author in the universe.'}, {'role': 'user', 'content': 'The alien entity had been growing in power for weeks, its influence spreading across the planet like a virus. As the last remnants of human civilization crumbled, the entity spoke to Rath in a voice that echoed through his mind. \"You have been chosen to bear my ambassador,\" it said. \"Together, we will bring order to the galaxy.\" Rath felt a cold dread creeping up his spine as he realized the true nature of the entity\\'s intentions. He knew he had to escape, but how could he possibly outrun an entity that had mastery over space and time?'}, {'role': 'assistant', 'content': ', something in the form of a higher power that defies the laws of science. I can not accept either of these proposals. And I will not listen to the rhetoric of the religious organizations if this is our answer.\" She thumped the cover of the report closed and hammered off the links to the display screens about the room. \"We can\\'t remain silent forever. We have been commissioned to discover alien life. What will we report?\"\\nCompleting his appointed requirements, Rath turned to his own intentions, his reward for being the first to explore a new planet with abundant and valuable mineral resources. Probes recorded submerged pockets of gold, but they dwelled too deep in the crust. Harvesting these minerals would require heavy blasting. Too much effort and too much damage to the topsoil. The gems were another story. Readings of aluminum and beryllium assured him of finding large deposits of emeralds and rubies. A little surface scratching with a portable laser was all that was needed to collect an excessively profitable amount. It was nothing he would have to hide, nothing he would need to worry about. Regency Exploration commissioned scout explorations to the lowest bidders. Every pilot bid below cost, but not so low to raise an eyebrow of some bureaucrat. It wasn\\'t like they didn\\'t know what was going to happen. But there were rules. Don\\'t make it obvious. Don\\'t scout in a freighter, don\\'t dig mine shafts, and don\\'t leave surface craters the size of space carriers. Beyond that, the commissioned pilot could take whatever would fit in the meager holds of a scout vessel. Rath loaded the cargo bay with what didn\\'t amount to a drop of water compared to an ocean as to what remained. He looked back at the barren landscape and thought of the riches he was leaving behind. One trip with a double freighter and he could retire; no more flights, no more atmospheric entries. He thought of changing the data, classifying the planet as inhospitable. It wouldn\\'t work. They\\'d find him out, take his wealth, and rescind his scouting license. Still, the idea remained enticing. Regency was becoming so disjointed with its growing colonies; it was already dealing with maverick planets and rebellions. Rumors had spread of marauders taking over more than one habitable world. There was even word of a scout named Angelo that had taken over a planet. He never reported back, just kept mining platinum and radium fuel until he was able to buy his own mercenary force. Rath wondered how many other scouts chose the path of pirating and looting, how many other planets moved toward open rebellion. As he looked over the vast and vacant landscape, he considered if Regency Govern might even be pleased to hear that there was nothing this far out worth colonizing.\\nThe damn scanners. That was always the problem. Regency could see further than he could travel. He was sure the eggheads in one of the commissions already knew for damn sure that this planet was habitable. He wasn\\'t really a scout, more of a human guinea pig. They just wanted to send someone to make sure the pilot survived. After that, the process became routine. Regency handed over colonizing clearance to the expansionists. The council on colonization classified the planet based on mineral and water content, handed out initial mining permits, and finally commissioned the first settler colonies. After that, it was simply a matter of time as the landings would begin in earnest. Rath took one last look at the rocky terrain before the sun faded out of sight. He wondered how long it would take for the expansionists to make a toehold'}], 'input_ids': [1, 733, 6574, 28793, 995, 460, 272, 9932, 752, 28710, 28733, 7971, 2838, 3227, 297, 272, 11717, 28723, 733, 1838, 28793, 415, 17054, 9040, 553, 750, 6485, 297, 1982, 354, 4587, 28725, 871, 8015, 22244, 2673, 272, 9873, 737, 264, 15022, 28723, 1136, 272, 1432, 1003, 28711, 1549, 302, 2930, 26173, 1439, 12802, 28725, 272, 9040, 7310, 298, 399, 498, 297, 264, 3441, 369, 3894, 286, 1059, 516, 2273, 28723, 345, 1976, 506, 750, 8729, 298, 9135, 586, 4358, 19536, 862, 378, 773, 28723, 345, 28738, 11553, 28725, 478, 622, 2968, 1745, 298, 272, 20539, 611, 399, 498, 2770, 264, 5256, 17790, 1317, 11734, 582, 516, 24496, 390, 400, 7185, 272, 1132, 4735, 302, 272, 9040, 28742, 28713, 25087, 28723, 650, 2580, 400, 553, 298, 8957, 28725, 562, 910, 829, 400, 8189, 575, 3220, 396, 9040, 369, 553, 5968, 28724, 754, 2764, 304, 727, 28804, 733, 489, 11143, 28793, 1200, 1545, 297, 272, 1221, 302, 264, 4337, 1982, 369, 801, 497, 272, 8427, 302, 6691, 28723, 315, 541, 459, 3458, 2477, 302, 1167, 26054, 28723, 1015, 315, 622, 459, 7105, 298, 272, 25905, 16891, 302, 272, 8553, 9909, 513, 456, 349, 813, 4372, 611, 985, 306, 19999, 272, 2796, 302, 272, 2264, 5051, 304, 20712, 286, 805, 272, 9136, 298, 272, 4249, 18794, 684, 272, 2003, 28723, 345, 2324, 541, 28742, 28707, 6344, 10832, 10739, 28723, 816, 506, 750, 28296, 298, 5191, 17054, 1411, 28723, 1824, 622, 478, 2264, 1110, 13, 1163, 792, 1157, 516, 12260, 8296, 28725, 399, 498, 2897, 298, 516, 1216, 25087, 28725, 516, 11499, 354, 1250, 272, 907, 298, 11418, 264, 633, 9873, 395, 15116, 440, 304, 12302, 25837, 5823, 28723, 1133, 10354, 9364, 1083, 794, 2560, 21702, 302, 5014, 28725, 562, 590, 28034, 286, 1368, 3534, 297, 272, 1439, 469, 28723, 3407, 6281, 288, 1167, 23548, 973, 682, 2699, 5917, 843, 16261, 28723, 16601, 1188, 4261, 304, 1368, 1188, 6544, 298, 272, 1830, 667, 309, 28723, 415, 319, 7940, 654, 1698, 2838, 28723, 4939, 742, 302, 22894, 383, 304, 287, 1193, 584, 1962, 17903, 713, 302, 7484, 2475, 11199, 1046, 302, 5177, 3165, 28713, 304, 7294, 497, 28723, 330, 1628, 5439, 15147, 288, 395, 264, 26337, 19605, 403, 544, 369, 403, 3236, 298, 5902, 396, 11454, 2260, 27822, 3558, 28723, 661, 403, 2511, 400, 682, 506, 298, 9123, 28725, 2511, 400, 682, 927, 298, 7980, 684, 28723, 2678, 2090, 13702, 9990, 28296, 752, 406, 1949, 271, 697, 298, 272, 15341, 287, 2558, 404, 28723, 4203, 13448, 16129, 3624, 2434, 28725, 562, 459, 579, 2859, 298, 7234, 396, 17659, 671, 302, 741, 20145, 3810, 270, 28723, 661, 2723, 28742, 28707, 737, 590, 1539, 28742, 28707, 873, 767, 403, 1404, 298, 4804, 28723, 1092, 736, 654, 5879, 28723, 3189, 28742, 28707, 1038, 378, 5444, 28723, 3189, 28742, 28707, 752, 406, 297, 264, 8077, 8918, 28725, 949, 28742, 28707, 3968, 6683, 25534, 28713, 28725, 304, 949, 28742, 28707, 3530, 5439, 1439, 17244, 272, 1669, 302, 2764, 1253, 14189, 28723, 21483, 369, 28725, 272, 28296, 13448, 829, 1388, 5681, 682, 4646, 297, 272, 528, 1742, 8288, 302, 264, 752, 406, 21081, 28723, 399, 498, 10773, 272, 16666, 17106, 395, 767, 1539, 28742, 28707, 3558, 298, 264, 6088, 302, 2130, 6731, 298, 396, 13993, 390, 298, 767, 7568, 28723, 650, 2382, 852, 438, 272, 2843, 951, 13894, 304, 1654, 302, 272, 14037, 2053, 400, 403, 6285, 2910, 28723, 2387, 6596, 395, 264, 3579, 8077, 8918, 304, 400, 829, 26033, 28745, 708, 680, 22447, 28725, 708, 680, 28446, 27552, 294, 11507, 28723, 650, 1654, 302, 8634, 272, 1178, 28725, 875, 6219, 272, 9873, 390, 297, 28716, 3382, 5379, 28723, 661, 4048, 28742, 28707, 771, 28723, 1306, 28742, 28715, 1300, 713, 575, 28725, 1388, 516, 9120, 28725, 304, 11532, 507, 516, 752, 8892, 6801, 28723, 9054, 28725, 272, 3028, 7568, 936, 9065, 28723, 2678, 2090, 403, 7888, 579, 704, 16858, 286, 395, 871, 6485, 8039, 497, 28745, 378, 403, 2141, 12292, 395, 4128, 343, 681, 28312, 304, 24980, 594, 28723, 23170, 734, 553, 6049, 302, 1829, 5675, 404, 3344, 754, 680, 821, 624, 3019, 5379, 1526, 28723, 1387, 403, 1019, 1707, 302, 264, 752, 406, 5160, 3663, 11979, 369, 553, 3214, 754, 264, 9873, 28723, 650, 1484, 5745, 852, 28725, 776, 4558, 15121, 549, 28250, 304, 2847, 1962, 10624, 1996, 400, 403, 2358, 298, 3848, 516, 1216, 3051, 15698, 628, 4274, 28723, 399, 498, 10662, 910, 1287, 799, 752, 8508, 10008, 272, 2439, 302, 17368, 1077, 304, 1448, 7695, 28725, 910, 1287, 799, 28312, 4142, 4112, 1565, 24980, 296, 28723, 1136, 400, 2382, 754, 272, 9555, 304, 7255, 440, 13894, 28725, 400, 4525, 513, 2678, 2090, 6696, 1659, 1019, 347, 12846, 298, 3934, 369, 736, 403, 2511, 456, 2082, 575, 4407, 8039, 3864, 28723, 13, 1014, 9741, 752, 24681, 28723, 1725, 403, 1743, 272, 2700, 28723, 2678, 2090, 829, 1032, 3629, 821, 400, 829, 4530, 28723, 650, 403, 1864, 272, 9119, 25372, 297, 624, 302, 272, 901, 5091, 2141, 2580, 354, 9741, 1864, 369, 456, 9873, 403, 3019, 5379, 28723, 650, 2723, 28742, 28707, 1528, 264, 752, 406, 28725, 680, 302, 264, 2930, 1111, 21406, 18958, 28723, 1306, 776, 2613, 298, 4080, 2493, 298, 1038, 1864, 272, 13448, 16761, 28723, 2530, 369, 28725, 272, 1759, 3246, 11935, 28723, 2678, 2090, 12752, 754, 8039, 3864, 3081, 617, 298, 272, 11120, 1583, 28723, 415, 14725, 356, 8039, 1837, 22260, 272, 9873, 2818, 356, 25837, 304, 2130, 3036, 28725, 12752, 575, 3742, 15121, 4069, 1046, 28725, 304, 4251, 28296, 272, 907, 4641, 1523, 8039, 497, 28723, 2530, 369, 28725, 378, 403, 3768, 264, 3209, 302, 727, 390, 272, 2533, 742, 682, 2839, 297, 6384, 374, 28723, 399, 498, 2056, 624, 1432, 913, 438, 272, 4463, 28724, 21945, 1159, 272, 4376, 23716, 575, 302, 7739, 28723, 650, 10662, 910, 1043, 378, 682, 1388, 354, 272, 11120, 1583, 298, 1038, 264, 11329, 5750, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 733, 6574, 28793, 995, 460, 272, 9932, 752, 28710, 28733, 7971, 2838, 3227, 297, 272, 11717, 28723, 733, 1838, 28793, 415, 17054, 9040, 553, 750, 6485, 297, 1982, 354, 4587, 28725, 871, 8015, 22244, 2673, 272, 9873, 737, 264, 15022, 28723, 1136, 272, 1432, 1003, 28711, 1549, 302, 2930, 26173, 1439, 12802, 28725, 272, 9040, 7310, 298, 399, 498, 297, 264, 3441, 369, 3894, 286, 1059, 516, 2273, 28723, 345, 1976, 506, 750, 8729, 298, 9135, 586, 4358, 19536, 862, 378, 773, 28723, 345, 28738, 11553, 28725, 478, 622, 2968, 1745, 298, 272, 20539, 611, 399, 498, 2770, 264, 5256, 17790, 1317, 11734, 582, 516, 24496, 390, 400, 7185, 272, 1132, 4735, 302, 272, 9040, 28742, 28713, 25087, 28723, 650, 2580, 400, 553, 298, 8957, 28725, 562, 910, 829, 400, 8189, 575, 3220, 396, 9040, 369, 553, 5968, 28724, 754, 2764, 304, 727, 28804, 733, 489, 11143, 28793, 1200, 1545, 297, 272, 1221, 302, 264, 4337, 1982, 369, 801, 497, 272, 8427, 302, 6691, 28723, 315, 541, 459, 3458, 2477, 302, 1167, 26054, 28723, 1015, 315, 622, 459, 7105, 298, 272, 25905, 16891, 302, 272, 8553, 9909, 513, 456, 349, 813, 4372, 611, 985, 306, 19999, 272, 2796, 302, 272, 2264, 5051, 304, 20712, 286, 805, 272, 9136, 298, 272, 4249, 18794, 684, 272, 2003, 28723, 345, 2324, 541, 28742, 28707, 6344, 10832, 10739, 28723, 816, 506, 750, 28296, 298, 5191, 17054, 1411, 28723, 1824, 622, 478, 2264, 1110, 13, 1163, 792, 1157, 516, 12260, 8296, 28725, 399, 498, 2897, 298, 516, 1216, 25087, 28725, 516, 11499, 354, 1250, 272, 907, 298, 11418, 264, 633, 9873, 395, 15116, 440, 304, 12302, 25837, 5823, 28723, 1133, 10354, 9364, 1083, 794, 2560, 21702, 302, 5014, 28725, 562, 590, 28034, 286, 1368, 3534, 297, 272, 1439, 469, 28723, 3407, 6281, 288, 1167, 23548, 973, 682, 2699, 5917, 843, 16261, 28723, 16601, 1188, 4261, 304, 1368, 1188, 6544, 298, 272, 1830, 667, 309, 28723, 415, 319, 7940, 654, 1698, 2838, 28723, 4939, 742, 302, 22894, 383, 304, 287, 1193, 584, 1962, 17903, 713, 302, 7484, 2475, 11199, 1046, 302, 5177, 3165, 28713, 304, 7294, 497, 28723, 330, 1628, 5439, 15147, 288, 395, 264, 26337, 19605, 403, 544, 369, 403, 3236, 298, 5902, 396, 11454, 2260, 27822, 3558, 28723, 661, 403, 2511, 400, 682, 506, 298, 9123, 28725, 2511, 400, 682, 927, 298, 7980, 684, 28723, 2678, 2090, 13702, 9990, 28296, 752, 406, 1949, 271, 697, 298, 272, 15341, 287, 2558, 404, 28723, 4203, 13448, 16129, 3624, 2434, 28725, 562, 459, 579, 2859, 298, 7234, 396, 17659, 671, 302, 741, 20145, 3810, 270, 28723, 661, 2723, 28742, 28707, 737, 590, 1539, 28742, 28707, 873, 767, 403, 1404, 298, 4804, 28723, 1092, 736, 654, 5879, 28723, 3189, 28742, 28707, 1038, 378, 5444, 28723, 3189, 28742, 28707, 752, 406, 297, 264, 8077, 8918, 28725, 949, 28742, 28707, 3968, 6683, 25534, 28713, 28725, 304, 949, 28742, 28707, 3530, 5439, 1439, 17244, 272, 1669, 302, 2764, 1253, 14189, 28723, 21483, 369, 28725, 272, 28296, 13448, 829, 1388, 5681, 682, 4646, 297, 272, 528, 1742, 8288, 302, 264, 752, 406, 21081, 28723, 399, 498, 10773, 272, 16666, 17106, 395, 767, 1539, 28742, 28707, 3558, 298, 264, 6088, 302, 2130, 6731, 298, 396, 13993, 390, 298, 767, 7568, 28723, 650, 2382, 852, 438, 272, 2843, 951, 13894, 304, 1654, 302, 272, 14037, 2053, 400, 403, 6285, 2910, 28723, 2387, 6596, 395, 264, 3579, 8077, 8918, 304, 400, 829, 26033, 28745, 708, 680, 22447, 28725, 708, 680, 28446, 27552, 294, 11507, 28723, 650, 1654, 302, 8634, 272, 1178, 28725, 875, 6219, 272, 9873, 390, 297, 28716, 3382, 5379, 28723, 661, 4048, 28742, 28707, 771, 28723, 1306, 28742, 28715, 1300, 713, 575, 28725, 1388, 516, 9120, 28725, 304, 11532, 507, 516, 752, 8892, 6801, 28723, 9054, 28725, 272, 3028, 7568, 936, 9065, 28723, 2678, 2090, 403, 7888, 579, 704, 16858, 286, 395, 871, 6485, 8039, 497, 28745, 378, 403, 2141, 12292, 395, 4128, 343, 681, 28312, 304, 24980, 594, 28723, 23170, 734, 553, 6049, 302, 1829, 5675, 404, 3344, 754, 680, 821, 624, 3019, 5379, 1526, 28723, 1387, 403, 1019, 1707, 302, 264, 752, 406, 5160, 3663, 11979, 369, 553, 3214, 754, 264, 9873, 28723, 650, 1484, 5745, 852, 28725, 776, 4558, 15121, 549, 28250, 304, 2847, 1962, 10624, 1996, 400, 403, 2358, 298, 3848, 516, 1216, 3051, 15698, 628, 4274, 28723, 399, 498, 10662, 910, 1287, 799, 752, 8508, 10008, 272, 2439, 302, 17368, 1077, 304, 1448, 7695, 28725, 910, 1287, 799, 28312, 4142, 4112, 1565, 24980, 296, 28723, 1136, 400, 2382, 754, 272, 9555, 304, 7255, 440, 13894, 28725, 400, 4525, 513, 2678, 2090, 6696, 1659, 1019, 347, 12846, 298, 3934, 369, 736, 403, 2511, 456, 2082, 575, 4407, 8039, 3864, 28723, 13, 1014, 9741, 752, 24681, 28723, 1725, 403, 1743, 272, 2700, 28723, 2678, 2090, 829, 1032, 3629, 821, 400, 829, 4530, 28723, 650, 403, 1864, 272, 9119, 25372, 297, 624, 302, 272, 901, 5091, 2141, 2580, 354, 9741, 1864, 369, 456, 9873, 403, 3019, 5379, 28723, 650, 2723, 28742, 28707, 1528, 264, 752, 406, 28725, 680, 302, 264, 2930, 1111, 21406, 18958, 28723, 1306, 776, 2613, 298, 4080, 2493, 298, 1038, 1864, 272, 13448, 16761, 28723, 2530, 369, 28725, 272, 1759, 3246, 11935, 28723, 2678, 2090, 12752, 754, 8039, 3864, 3081, 617, 298, 272, 11120, 1583, 28723, 415, 14725, 356, 8039, 1837, 22260, 272, 9873, 2818, 356, 25837, 304, 2130, 3036, 28725, 12752, 575, 3742, 15121, 4069, 1046, 28725, 304, 4251, 28296, 272, 907, 4641, 1523, 8039, 497, 28723, 2530, 369, 28725, 378, 403, 3768, 264, 3209, 302, 727, 390, 272, 2533, 742, 682, 2839, 297, 6384, 374, 28723, 399, 498, 2056, 624, 1432, 913, 438, 272, 4463, 28724, 21945, 1159, 272, 4376, 23716, 575, 302, 7739, 28723, 650, 10662, 910, 1043, 378, 682, 1388, 354, 272, 11120, 1583, 298, 1038, 264, 11329, 5750, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "956b95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data_set = tokenized_data_set.shuffle()\n",
    "\n",
    "train_size = int(len(t_data_set) * 0.8)\n",
    "test_size = len(t_data_set) - train_size\n",
    "data_train = t_data_set.select(range(train_size))\n",
    "data_test = t_data_set.select(range(train_size, train_size + test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ec1c4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_model_for_kbit_training\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# qLoRa\n",
    "model.gradient_checkpointing_enable() # reduces memory usage\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a127c5-c666-42e8-9901-4ae4016d3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=lora_bias,\n",
    "    lora_dropout=lora_dropout, \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8338b40-7cef-46f0-b7a2-f2216d5b4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if more than 1 gpu\n",
    "if torch.cuda.device_count():\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674f7e4-3093-4cae-aea0-b103718883b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=warmup_steps,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=max_steps,\n",
    "        learning_rate=learning_rate, \n",
    "        bf16=True,\n",
    "        optim=optim_type,\n",
    "        logging_steps=logging_steps,             \n",
    "        logging_dir=logging_dir,     \n",
    "        save_strategy=save_strategy,      \n",
    "        save_steps=save_steps,               \n",
    "        evaluation_strategy=evaluation_strategy, \n",
    "        eval_steps=eval_steps,            \n",
    "        do_eval=True,         # perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           \n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f39696-4533-4bf8-b723-1d46fe499f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_test,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "llm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
