{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac877a3-fe64-4e0e-88f9-ec2c6ada66da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\coding\\ai_learnoor\\learner_env\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:149: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import runpod\n",
    "from text_generation import Client\n",
    "import json\n",
    "import re\n",
    "import tiktoken\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33eb748a-c51b-4b52-a9d9-e8139cc88c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f72e8f9-59c8-462b-a777-850037665b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_whitespace(text):\n",
    "        text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "        text = re.sub(r' {2,}', '\\n', text)\n",
    "        text = text.strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93921088-ff3c-4f75-addd-52e6ee678ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3642\n"
     ]
    }
   ],
   "source": [
    "def load_data(data):\n",
    "    with open(data, 'r', encoding='utf-8') as f:\n",
    "        story = f.read()\n",
    "    \n",
    "    clean_story = clean_whitespace(story)\n",
    "    \n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = encoding.encode(clean_story)\n",
    "    print(len(num_tokens))\n",
    "\n",
    "    segments = {}\n",
    "    current_segment = []\n",
    "    seg_num = 1\n",
    "    \n",
    "    # will loop untill dict has 1000 tokens\n",
    "    for token in num_tokens:\n",
    "        current_segment.append(token)\n",
    "        \n",
    "        # if >= 1000 will add this batch to dict\n",
    "        if len(current_segment) >= 1000:\n",
    "            segment_text = encoding.decode(current_segment)\n",
    "            segments[f'Segment: {seg_num}'] = segment_text\n",
    "            current_segment = []\n",
    "            seg_num += 1\n",
    "            \n",
    "    # grabs remaining tokens        \n",
    "    if current_segment:\n",
    "        segment_text = encoding.decode(current_segment)\n",
    "        segments[f'Segment: {seg_num}'] = segment_text\n",
    "     \n",
    "    return segments\n",
    "\n",
    "segmented_data = load_data(\"D:\\coding\\llms\\sci_storys\\story4.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "771ac104-0328-4a0a-8799-7d935ca5697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(input_text, api_key):\n",
    "    url = \"https://api.runpod.ai/v2/llama2-13b-chat/runsync\"\n",
    "\n",
    "    prompt= f\"\"\"\n",
    "    Based on the following story segment '{input_text}', directly create a brief sci-fi story prompt. \n",
    "    Start the prompt immediately without any introduction, explanation, or additional words. \n",
    "    End the prompt without any concluding remarks or questions. Provide only the prompt, exactly as requested, nothing more, nothing less.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    payload = { \"input\": {\n",
    "        \"prompt\": prompt,\n",
    "        \"sampling_params\": {\n",
    "            \"max_tokens\": 1000,\n",
    "            \"n\": 1,\n",
    "            \"best_of\": None,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"frequency_penalty\": 0.2,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 1,\n",
    "            \"top_k\": -1,\n",
    "            \"use_beam_search\": False,\n",
    "            \"ignore_eos\": False,\n",
    "            \"logprobs\": None\n",
    "        }\n",
    "    } }\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": api_key\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    response_data = response.json()\n",
    "\n",
    "    job_id = response_data.get('id')\n",
    "    return job_id  \n",
    "    \n",
    "\n",
    "def poll_for_result(request_id, api_key, interval=5):\n",
    "    status_url = f\"https://api.runpod.ai/v2/llama2-13b-chat/status/{request_id}\"  \n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"authorization\": api_key\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(status_url, headers=headers)\n",
    "        result = response.json()\n",
    "\n",
    "        if result['status'] == 'COMPLETED':\n",
    "            output = result.get('output')\n",
    "            return output  \n",
    "        elif result['status'] in ['FAILED', 'ERROR']:\n",
    "            print(\"Error or Failed Status:\", result)\n",
    "            return result\n",
    "\n",
    "        time.sleep(interval)  # Wait before polling again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b4812-f3ea-4b16-acd7-726c423cfe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_to_prompt(segmented_data):\n",
    "    ''' \n",
    "    takes segmented input (dictionary) loops through,\n",
    "    \"gen_prompt\" & \"poll_for_result\" functions\n",
    "    takes llm gen prompt from output\n",
    "    and places in new prompt with input text\n",
    "    ''' \n",
    "    i = 1\n",
    "    json_objects = []\n",
    "    # loop through segmented outputs\n",
    "    for _, text in segmented_data.items():\n",
    "        input_text = text\n",
    "        job_id = gen_prompt(input_text, api_key)\n",
    "        if job_id:\n",
    "            output = poll_for_result(job_id, api_key)\n",
    "            \n",
    "            text = output['text'][0] if output['text'] else None\n",
    "            extracted_text = \"\"\n",
    "            \n",
    "            # extract text between \\n\\n\n",
    "            if text:\n",
    "                matches = re.findall(r\"\\n\\n(.*?)(?:\\n\\n|$)\", text, re.DOTALL)\n",
    "                if matches:\n",
    "                    extracted_text = matches[0].strip()\n",
    "                    json_obj = {\n",
    "                      \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are the greatest sci-fi story author in the universe.\"},\n",
    "                        {\"role\": \"user\", \"content\": extracted_text},\n",
    "                        {\"role\": \"assistant\", \"content\": input_text}\n",
    "                      ]\n",
    "                    }\n",
    "                    json_objects.append(json_obj) \n",
    "        i += 1\n",
    "\n",
    "        return json_objects\n",
    "json_output = extract_to_prompt(segmented_data)\n",
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a35d307-5d9c-49f0-ab4b-65b35398b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexi and Steve were on the run from an angry mob after a bizarre incident involving the Pope and a mysterious discovery. They had holed up in a run-down safe house on the outskirts of town, but they knew it was only a matter of time before the mob found them. As they huddled together on a dirty couch, watching the news in disbelief, they suddenly heard a strange noise coming from outside. It sounded like something was trying to get in.\n"
     ]
    }
   ],
   "source": [
    "text = output['text'][0] if output['text'] else None\n",
    "# extract text between \\n\\n\n",
    "if text:\n",
    "    matches = re.findall(r\"\\n\\n(.*?)(?:\\n\\n|$)\", text, re.DOTALL)\n",
    "    if matches:\n",
    "        extracted_text = matches[0].strip()\n",
    "\n",
    "\n",
    "# segments = extracted_text.split('\\n')\n",
    "# extracted_prompt = segments[2].strip() if len(segments) > 1 else None    \n",
    "# extracted_prompt = extracted_prompt.strip(\"'\")\n",
    "\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "81f4060d-7501-4449-b05d-4fb675def9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'You are the greatest sci-fi story teller the world has seen.'},\n",
       "  {'role': 'user',\n",
       "   'content': \"What if Lexi's discovery led to a catastrophic event that threatened the fabric of reality? Could she use her knowledge of the Vacuity Machine to prevent the impending doom, or would the power of the black holes prove too great for her to control?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'It was approaching 2am when Lexi made the discovery that would fracture the world. She skulled the rest of her black coffee and sat the paper cup down on the smooth, white table. She was at the university working on her PhD thesis. The research involved creating controlled, miniature black holes in a Vacuity Machine and then testing its potential for hyper-space travel.'}]}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_obj = {\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are the greatest sci-fi story author in the universe.\"},\n",
    "    {\"role\": \"user\", \"content\": extracted_text},\n",
    "    {\"role\": \"assistant\", \"content\": input_text}\n",
    "  ]\n",
    "}\n",
    "json_obj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "llm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
